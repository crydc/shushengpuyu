# 书生浦语
nternLM 是在过万亿 token 数据上训练的多语千亿参数基座模型。通过多阶段的渐进式训练，InternLM 基座模型具有较高的知识水平，在中英文阅读理解、推理任务等需要较强思维能力的场景下性能优秀，在多种面向人类设计的综合性考试中表现突出。在此基础上，通过高质量的人类标注对话数据结合 RLHF 等技术，使得 InternLM 可以在与人类对话时响应复杂指令，并且表现出符合人类道德与价值观的回复
# 书生浦语历程
![image](https://github.com/crydc/shushengpuyu/assets/97872790/fc3838c9-ecf6-44a4-9c7d-ae6197872182)
# 书生浦语体系
![image](https://github.com/crydc/shushengpuyu/assets/97872790/2d75b36d-7d9d-42bb-a831-4b34cb1d66e1)
# 应用流程
![image](https://github.com/crydc/shushengpuyu/assets/97872790/f7170fe9-5999-4b1f-bb93-cae12a0d19ea)
# 模型性能
![image](https://github.com/crydc/shushengpuyu/assets/97872790/d7e7dfde-9199-4ee8-b227-a706465f4c66)
### 书生浦语支持多种类行的智能体，例如ReAct等，同时支持多种大语言模型，还拥有丰富的工具，一键远程部署。
书生浦语采用InternEvo训练框架且支持多GPU，预训练数据包括文本、代码和长文本数据，使用监督式微调 (SFT)和条件在线强化学习 (COOL RLHF)的对齐方式，InternLM2还通过Group Query Attention (GQA)和更大的上下文窗口（从4k到32k令牌）来提高长文本处理能力。

